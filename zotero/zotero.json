[
  {"id":"arjovskyWassersteinGAN2017","abstract":"We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Arjovsky","given":"Martin"},{"family":"Chintala","given":"Soumith"},{"family":"Bottou","given":"Léon"}],"citation-key":"arjovskyWassersteinGAN2017","DOI":"10.48550/arXiv.1701.07875","issued":{"date-parts":[[2017,12,6]]},"note":"3146 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1701.07875","publisher":"arXiv","source":"arXiv.org","title":"Wasserstein GAN","title-short":"WGAN","type":"article","URL":"http://arxiv.org/abs/1701.07875"},
  {"id":"brockLargeScaleGAN2019","abstract":"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.","accessed":{"date-parts":[[2023,4,9]]},"author":[{"family":"Brock","given":"Andrew"},{"family":"Donahue","given":"Jeff"},{"family":"Simonyan","given":"Karen"}],"citation-key":"brockLargeScaleGAN2019","DOI":"10.48550/arXiv.1809.11096","issued":{"date-parts":[[2019,2,25]]},"note":"3563 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1809.11096","publisher":"arXiv","source":"arXiv.org","title":"Large Scale GAN Training for High Fidelity Natural Image Synthesis","title-short":"BigGAN","type":"article","URL":"http://arxiv.org/abs/1809.11096","version":"2"},
  {"id":"donahueAdversarialAudioSynthesis2019","abstract":"Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Donahue","given":"Chris"},{"family":"McAuley","given":"Julian"},{"family":"Puckette","given":"Miller"}],"citation-key":"donahueAdversarialAudioSynthesis2019","DOI":"10.48550/arXiv.1802.04208","issued":{"date-parts":[[2019,2,8]]},"note":"420 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1802.04208","publisher":"arXiv","source":"arXiv.org","title":"Adversarial Audio Synthesis","title-short":"WaveGAN","type":"article","URL":"http://arxiv.org/abs/1802.04208"},
  {"id":"fregierMind2MindTransferLearning2021a","abstract":"Training generative adversarial networks (GANs) on high quality (HQ) images involves important computing resources. This requirement represents a bottleneck for the development of applications of GANs. We propose a transfer learning technique for GANs that signiﬁcantly reduces training time. Our approach consists of freezing the low-level layers of both the critic and generator of the original GAN. We assume an autoencoder constraint in order to ensure the compatibility of the internal representations of the critic and the generator. This assumption explains the gain in training time as it enables us to bypass the low-level layers during the forward and backward passes. We compare our method to baselines and observe a signiﬁcant acceleration of the training. It can reach two orders of magnitude on HQ datasets when compared with StyleGAN. We prove rigorously, within the framework of optimal transport, a theorem ensuring the convergence of the learning of the transferred GAN. We moreover provide a precise bound for the convergence of the training in terms of the distance between the source and target dataset.","accessed":{"date-parts":[[2023,4,1]]},"author":[{"family":"Frégier","given":"Yaël"},{"family":"Gouray","given":"Jean-Baptiste"}],"citation-key":"fregierMind2MindTransferLearning2021a","DOI":"10.1007/978-3-030-80209-7","issued":{"date-parts":[[2021]]},"language":"en","note":"15 citations (Semantic Scholar/arXiv) [2023-04-10]","source":"arXiv.org","title":"Mind2Mind : transfer learning for GANs","title-short":"GAN-TL","type":"book","URL":"http://arxiv.org/abs/1906.11613","volume":"12829"},
  {"id":"goodfellowGenerativeAdversarialNetworks2014","abstract":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","accessed":{"date-parts":[[2023,4,1]]},"author":[{"family":"Goodfellow","given":"Ian J."},{"family":"Pouget-Abadie","given":"Jean"},{"family":"Mirza","given":"Mehdi"},{"family":"Xu","given":"Bing"},{"family":"Warde-Farley","given":"David"},{"family":"Ozair","given":"Sherjil"},{"family":"Courville","given":"Aaron"},{"family":"Bengio","given":"Yoshua"}],"citation-key":"goodfellowGenerativeAdversarialNetworks2014","DOI":"10.48550/arXiv.1406.2661","issued":{"date-parts":[[2014,6,10]]},"number":"arXiv:1406.2661","publisher":"arXiv","source":"arXiv.org","title":"Generative Adversarial Networks","title-short":"GAN","type":"article","URL":"http://arxiv.org/abs/1406.2661"},
  {"id":"isolaImagetoImageTranslationConditional2018","abstract":"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Isola","given":"Phillip"},{"family":"Zhu","given":"Jun-Yan"},{"family":"Zhou","given":"Tinghui"},{"family":"Efros","given":"Alexei A."}],"citation-key":"isolaImagetoImageTranslationConditional2018","DOI":"10.48550/arXiv.1611.07004","issued":{"date-parts":[[2018,11,26]]},"note":"9995 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1611.07004","publisher":"arXiv","source":"arXiv.org","title":"Image-to-Image Translation with Conditional Adversarial Networks","title-short":"cGAN","type":"article","URL":"http://arxiv.org/abs/1611.07004","version":"3"},
  {"id":"karrasAnalyzingImprovingImage2020","abstract":"The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Karras","given":"Tero"},{"family":"Laine","given":"Samuli"},{"family":"Aittala","given":"Miika"},{"family":"Hellsten","given":"Janne"},{"family":"Lehtinen","given":"Jaakko"},{"family":"Aila","given":"Timo"}],"citation-key":"karrasAnalyzingImprovingImage2020","DOI":"10.48550/arXiv.1912.04958","issued":{"date-parts":[[2020,3,23]]},"note":"2872 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1912.04958","publisher":"arXiv","source":"arXiv.org","title":"Analyzing and Improving the Image Quality of StyleGAN","title-short":"StyleGAN2","type":"article","URL":"http://arxiv.org/abs/1912.04958","version":"2"},
  {"id":"karrasStyleBasedGeneratorArchitecture2019","abstract":"We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Karras","given":"Tero"},{"family":"Laine","given":"Samuli"},{"family":"Aila","given":"Timo"}],"citation-key":"karrasStyleBasedGeneratorArchitecture2019","DOI":"10.48550/arXiv.1812.04948","issued":{"date-parts":[[2019,3,29]]},"note":"5401 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1812.04948","publisher":"arXiv","source":"arXiv.org","title":"A Style-Based Generator Architecture for Generative Adversarial Networks","title-short":"StyleGAN","type":"article","URL":"http://arxiv.org/abs/1812.04948","version":"3"},
  {"id":"larsenAutoencodingPixelsUsing2016","abstract":"We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.","accessed":{"date-parts":[[2023,4,10]]},"author":[{"family":"Larsen","given":"Anders Boesen Lindbo"},{"family":"Sønderby","given":"Søren Kaae"},{"family":"Larochelle","given":"Hugo"},{"family":"Winther","given":"Ole"}],"citation-key":"larsenAutoencodingPixelsUsing2016","DOI":"10.48550/arXiv.1512.09300","issued":{"date-parts":[[2016,2,10]]},"number":"arXiv:1512.09300","publisher":"arXiv","source":"arXiv.org","title":"Autoencoding beyond pixels using a learned similarity metric","title-short":"VAE-GAN","type":"article","URL":"http://arxiv.org/abs/1512.09300"},
  {"id":"maoLeastSquaresGenerative2017","abstract":"Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Mao","given":"Xudong"},{"family":"Li","given":"Qing"},{"family":"Xie","given":"Haoran"},{"family":"Lau","given":"Raymond Y. K."},{"family":"Wang","given":"Zhen"},{"family":"Smolley","given":"Stephen Paul"}],"citation-key":"maoLeastSquaresGenerative2017","DOI":"10.48550/arXiv.1611.04076","issued":{"date-parts":[[2017,4,5]]},"note":"3479 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1611.04076","publisher":"arXiv","source":"arXiv.org","title":"Least Squares Generative Adversarial Networks","title-short":"LSGAN","type":"article","URL":"http://arxiv.org/abs/1611.04076"},
  {"id":"radfordUnsupervisedRepresentationLearning2016","abstract":"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Radford","given":"Alec"},{"family":"Metz","given":"Luke"},{"family":"Chintala","given":"Soumith"}],"citation-key":"radfordUnsupervisedRepresentationLearning2016","DOI":"10.48550/arXiv.1511.06434","issued":{"date-parts":[[2016,1,7]]},"note":"9996 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1511.06434","publisher":"arXiv","source":"arXiv.org","title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","title-short":"DCGAN","type":"article","URL":"http://arxiv.org/abs/1511.06434","version":"2"},
  {"id":"razghandiVariationalAutoencoderGenerative2022","abstract":"Data is the fuel of data science and machine learning techniques for smart grid applications, similar to many other fields. However, the availability of data can be an issue due to privacy concerns, data size, data quality, and so on. To this end, in this paper, we propose a Variational AutoEncoder Generative Adversarial Network (VAE-GAN) as a smart grid data generative model which is capable of learning various types of data distributions and generating plausible samples from the same distribution without performing any prior analysis on the data before the training phase.We compared the Kullback-Leibler (KL) divergence, maximum mean discrepancy (MMD), and Wasserstein distance between the synthetic data (electrical load and PV production) distribution generated by the proposed model, vanilla GAN network, and the real data distribution, to evaluate the performance of our model. Furthermore, we used five key statistical parameters to describe the smart grid data distribution and compared them between synthetic data generated by both models and real data. Experiments indicate that the proposed synthetic data generative model outperforms the vanilla GAN network. The distribution of VAE-GAN synthetic data is the most comparable to that of real data.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Razghandi","given":"Mina"},{"family":"Zhou","given":"Hao"},{"family":"Erol-Kantarci","given":"Melike"},{"family":"Turgut","given":"Damla"}],"citation-key":"razghandiVariationalAutoencoderGenerative2022","DOI":"10.48550/arXiv.2201.07387","issued":{"date-parts":[[2022,1,18]]},"note":"2 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:2201.07387","publisher":"arXiv","source":"arXiv.org","title":"Variational Autoencoder Generative Adversarial Network for Synthetic Data Generation in Smart Home","title-short":"VAE-GAN","type":"article","URL":"http://arxiv.org/abs/2201.07387"},
  {"id":"schmidhuberGenerativeAdversarialNetworks2020","abstract":"I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game.","accessed":{"date-parts":[[2023,4,9]]},"author":[{"family":"Schmidhuber","given":"Juergen"}],"citation-key":"schmidhuberGenerativeAdversarialNetworks2020","DOI":"10.48550/arXiv.1906.04493","issued":{"date-parts":[[2020,4,22]]},"note":"39 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1906.04493","publisher":"arXiv","source":"arXiv.org","title":"Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991)","title-short":"GAN-Predict-Min","type":"article","URL":"http://arxiv.org/abs/1906.04493"},
  {"id":"wangDiffusionGANTrainingGANs2022","abstract":"Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator's timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Wang","given":"Zhendong"},{"family":"Zheng","given":"Huangjie"},{"family":"He","given":"Pengcheng"},{"family":"Chen","given":"Weizhu"},{"family":"Zhou","given":"Mingyuan"}],"citation-key":"wangDiffusionGANTrainingGANs2022","DOI":"10.48550/arXiv.2206.02262","issued":{"date-parts":[[2022,10,8]]},"note":"26 citations (Semantic Scholar/arXiv) [2023-04-10]\n26 citations (Semantic Scholar/DOI) [2023-04-10]","number":"arXiv:2206.02262","publisher":"arXiv","source":"arXiv.org","title":"Diffusion-GAN: Training GANs with Diffusion","title-short":"Diffusion-GAN","type":"article","URL":"http://arxiv.org/abs/2206.02262"},
  {"id":"wangTransferringGANsGenerating2018a","abstract":"Transferring knowledge of pre-trained networks to new domains by means of ﬁne-tuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pre-trained networks can shorten the convergence time and can signiﬁcantly improve the quality of the generated images, especially when target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pre-trained model was trained without conditioning. Our results also suggest that density is more important than diversity and a dataset with one or few densely sampled classes is a better source model than more diverse datasets such as ImageNet or Places.","accessed":{"date-parts":[[2023,4,1]]},"author":[{"family":"Wang","given":"Yaxing"},{"family":"Wu","given":"Chenshen"},{"family":"Herranz","given":"Luis"},{"family":"Weijer","given":"Joost","non-dropping-particle":"van de"},{"family":"Gonzalez-Garcia","given":"Abel"},{"family":"Raducanu","given":"Bogdan"}],"citation-key":"wangTransferringGANsGenerating2018a","issued":{"date-parts":[[2018,10,2]]},"language":"en","note":"182 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1805.01677","publisher":"arXiv","source":"arXiv.org","title":"Transferring GANs: generating images from limited data","title-short":"GAN-TL","type":"article","URL":"http://arxiv.org/abs/1805.01677"},
  {"id":"yaziciAutoregressiveGenerativeAdversarial2022","abstract":"Generative Adversarial Networks (GANs) learn a generative model by playing an adversarial game between a generator and an auxiliary discriminator, which classifies data samples vs. generated ones. However, it does not explicitly model feature co-occurrences in samples. In this paper, we propose a novel Autoregressive Generative Adversarial Network (ARGAN), that models the latent distribution of data using an autoregressive model, rather than relying on binary classification of samples into data/generated categories. In this way, feature co-occurrences in samples can be more efficiently captured. Our model was evaluated on two widely used datasets: CIFAR-10 and STL-10. Its performance is competitive with respect to other GAN models both quantitatively and qualitatively.","accessed":{"date-parts":[[2023,4,10]]},"author":[{"family":"Yazici","given":"Yasin"},{"family":"Yap","given":"Kim-Hui"},{"family":"Winkler","given":"Stefan"}],"citation-key":"yaziciAutoregressiveGenerativeAdversarial2022","issued":{"date-parts":[[2022,2,10]]},"language":"en","source":"openreview.net","title":"Autoregressive Generative Adversarial Networks","title-short":"ARGAN","type":"article-journal","URL":"https://openreview.net/forum?id=rJWrK9lAb"},
  {"id":"zhangSelfAttentionGenerativeAdversarial2019","abstract":"In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Zhang","given":"Han"},{"family":"Goodfellow","given":"Ian"},{"family":"Metaxas","given":"Dimitris"},{"family":"Odena","given":"Augustus"}],"citation-key":"zhangSelfAttentionGenerativeAdversarial2019","DOI":"10.48550/arXiv.1805.08318","issued":{"date-parts":[[2019,6,14]]},"note":"2588 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1805.08318","publisher":"arXiv","source":"arXiv.org","title":"Self-Attention Generative Adversarial Networks","title-short":"SAGAN","type":"article","URL":"http://arxiv.org/abs/1805.08318","version":"2"},
  {"id":"zhangStackGANTextPhotorealistic2017","abstract":"Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Zhang","given":"Han"},{"family":"Xu","given":"Tao"},{"family":"Li","given":"Hongsheng"},{"family":"Zhang","given":"Shaoting"},{"family":"Wang","given":"Xiaogang"},{"family":"Huang","given":"Xiaolei"},{"family":"Metaxas","given":"Dimitris"}],"citation-key":"zhangStackGANTextPhotorealistic2017","DOI":"10.48550/arXiv.1612.03242","issued":{"date-parts":[[2017,8,4]]},"note":"2157 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1612.03242","publisher":"arXiv","source":"arXiv.org","title":"StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks","title-short":"StackGAN","type":"article","URL":"http://arxiv.org/abs/1612.03242"},
  {"id":"zhuUnpairedImagetoImageTranslation2020","abstract":"Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.","accessed":{"date-parts":[[2023,4,8]]},"author":[{"family":"Zhu","given":"Jun-Yan"},{"family":"Park","given":"Taesung"},{"family":"Isola","given":"Phillip"},{"family":"Efros","given":"Alexei A."}],"citation-key":"zhuUnpairedImagetoImageTranslation2020","DOI":"10.48550/arXiv.1703.10593","issued":{"date-parts":[[2020,8,24]]},"note":"3393 citations (Semantic Scholar/arXiv) [2023-04-10]","number":"arXiv:1703.10593","publisher":"arXiv","source":"arXiv.org","title":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks","title-short":"CycleGAN","type":"article","URL":"http://arxiv.org/abs/1703.10593","version":"7"}
]
